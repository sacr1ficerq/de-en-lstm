# Отчет о bhw2

## Загрузка и работа с данными

В своей работе я много времени уделил экспериментам с данными и созданию комфортной экосистемы абстракций для работы с ними. Для сравнения можно открыть мой код из чекпоинта `dataset.py` и финальный код, и посмотреть, насколько далеко я продвинулся в работе с абстракциями над датасетом и загрузчиком.

Данные представляют собой набор из предложений из двух языков. Можно выделить два гиперпараметра модели, которые играют ключевую роль в баллансе между размерностью входных данных и доступной видеопамятью и временем работы. Эти два параметра - `vocab_size` и `max_lengh`.

55% Всех слов в `src` встречаются один раз. Это выглядит как серьезная цифра, однако доля эти слов следующая

| кол-во повторений | доля в словаре | доля от всех слов |
| :------------------------------: | :------------------------: | :-----------------------------: |
|                1                |            55%            |              1.9%              |
|                2                |           13.64%           |              0.94%              |
|                3                |           6.65%           |              0.69%              |
|                4                |           3.94%           |              0.4%              |
|                <5                |           79.76%           |              4.07%              |

Для `trg` доля выглядит так:

| кол-во повторений | доля в словаре | доля от всех слов |
| -------------------------------- | -------------------------- | ------------------------------- |
| 1                                | 39.56%                     | 0.58%                           |
| 2                                | 13.83%                     | 0.4%                            |
| 3                                | 7.65%                      | 0.34%                           |
| 4                                | 5.14%                      | 0.3%                            |
| 5                                | 3.63%                      | 0.27%                           |
| <5                               | 66.17%                     | 1.62%                           |
| <6                               | 69.8%                      | 1.88%                           |

Я думаю оптимально для нашей задачи будет брать слова, встречающиеся 5 и более раз, это отсечет всего лишь 4% выборки, но при этом разительно понизит количество параметров в нашей модели и снизит время обучения.

Второй параметр - `max_len`. Максимальный размер предложения. Я начинал с того, что не обрезал данные совсем и для `batch_size=32`. Получал время эпохи около 5 минут для `LSTM.` C 15млн параметрами, что довольно много, к тому же у меня постоянно заканчивалис 11 гигабайт видеопамяти, и видимо torch начинал двигать батчи туда-сюда и время эпох значительно увеличивалось.

Основные проблемы от оставления всех длин последовательностей появляются когда мы дополняем последвательности в батчах паддингами для парамельного умножения в `forward()`. Я решил протестировать идею сортировки данных по длине последовательностей в `src` и оставления последовательностей до 64. Я понимал, что возможно градиенты перестанут быть некоррелированными и модель может быстро переобучитбся на батчи, но все равно хотелось проверить сколько времени потратится на эпоху с таким подходом. И время эохи упало до 1м 45с, но моя модель быстро переобучилась и все стало плохо.

![Sorted Batches](lstm-sorted-batches-losses.png)

Остался только один вариант - мириться с тем, что надо обрезать значительную часть выборки, либо как-то делить предложения на несколько частей.

В выборке `src` длины последовательностей следующие

| длина  | доля потери слов |
| ----------- | ------------------------------ |
| $\leq$ 64 | 1.08%                          |
| $\leq$ 48 | 3.98%                          |
| $\leq$ 42 | 5.47%                          |
| $\leq$ 32 | 7.06%                          |

Я думаю оптимально брать около 48

## LSTM

Архитектура, которую я обучил к чекпоинту лежит в файле `lstm.py`. 

У меня не сохранились графики обучения и ошибки до этой модели, но они не сильно интересны, так как только на ней я пробил 20 BLEU4.

У модели следующая архитектура:

```python
  (src_embedding): Embedding(55315, 64)
  (trg_embedding): Embedding(34047, 64)
  (encoder): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)
  (encoder_output_proj): Linear(in_features=256, out_features=128, bias=True)
  (decoder): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.1)
  (encoder_hidden_proj): (
    (0-1): 2 x Linear(in_features=256, out_features=128, bias=True)
  )
  (encoder_cell_proj): (
    (0-1): 2 x Linear(in_features=256, out_features=128, bias=True)
  )
  (fc): Linear(in_features=256, out_features=34047, bias=True)
```

А также внутри форварда я пропихиваю 


```python-repl
# attention
energy = torch.bmm(decoder_outputs, encoder_outputs.transpose(1,2))  
attention = F.softmax(energy,dim=-1)
context = torch.bmm(attention, encoder_outputs)
combined = torch.cat([decoder_outputs, context],dim=2)
logits =self.fc(combined)
```
