# Архитектура

Базовая архитектура модели, к которой я пришел выглядит следующим образом.

## Структура

1. **Encoder**:
   - Bidirectional LSTM
   - Выходы энкодера проецируются в пространство декодера `hidden_size*2 -> hidden_size`
   - LayerNorm для скрытых состояний и выхода
   - Dropout для выхода

2. **Decoder**:
   - LSTM
   - LayerNorm для скрытых состояний и выхода
   - Dropout для выхода

3. **Классификатор**:
   - Полносвязный слой (`hidden_size*2 → trg_vocab_size`)

### forward

1. Индексы в словаре src -> эмбеддинги
2. Эмбеддинги -> Encoder -> выходы, (скрытые состояния)
3. Проекция выходов энкодера в пространство соразмерное выходам декодера
(понижение размерности в два раза через линейный слой для attention)
4. Проекция скрытых состояний энкодера в пространство скрытых состояний декодера
(конкатенация в единую матрицу проекций скрытых состояний от обоих проходов)
5. Индексы в словаре trg -> эмбеддинги
6. Эмбеддинги, (скрытые состояния энкодера) -> Decoder -> выходы, (скрытые состояния)
7. (выходы декодера, выходы энкодера) -> energy
(матрица скалярных произведений строк выходов энкодера и декодера)
8. energy -> Softmax -> attention + максирование паддингов
*(баг, на который я потратил 3 часа своей жизни, так как отлаживал на маленьком батч сайзе, на котором не появлялось падингов)*
9. Выходы экодера -> маска attention -> контекст
10. [выходы декодера, контекст] -> Классификатор -> логиты

### inference

1. src -> Embdeddings
2. Embdeddings -> Encoder -> projections+normalizations -> encoder_output, (hidden, cell)
3. Пошаговая генерация:
   - последний токен (начиная с \<EOS\>) -> Embeddings
   - Embeddings, (hidden, cell) -> Decoder -> normalization -> decoder_output
   - encoder_output, decoder_output -> Attention -> Context -> Логиты
   - Логиты -> токен с максимальной вероятностью
*нюанс: если перевод сильно короче изначального, нас экспоненциально штрафуют (смотри часть про bleu), поэтому вместо удаления UNK, лучше сгенерировать другой токен, достигаем этого обнулением вероятности в логитах (logits[unk\] = -1e10)*