{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195915/195915 [00:29<00:00, 6596.41it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 7303.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from main import *\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<dataset.Vocab at 0x283491cc830>,\n",
       " <dataset.Vocab at 0x28348fd2490>,\n",
       " <dataset.TranslationDataset at 0x2836d70ea50>,\n",
       " <dataset.TranslationDataset at 0x2836d6b7250>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_src, vocab_trg, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import TranslationDataset, Vocab, TrainDataLoader, TestDataLoader\n",
    "from config import filenames, folders\n",
    "from lstm3 import LSTM_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_3(\n",
      "  (src_embedding): Embedding(21665, 120, padding_idx=1)\n",
      "  (trg_embedding): Embedding(18710, 120, padding_idx=1)\n",
      "  (emb_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (enc_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (dec_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (attention_dropout): Dropout(p=0.1, inplace=False)\n",
      "  (encoder): LSTM(120, 360, num_layers=3, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (encoder_output_proj): Linear(in_features=720, out_features=360, bias=True)\n",
      "  (decoder): LSTM(120, 360, num_layers=3, batch_first=True, dropout=0.1)\n",
      "  (encoder_hidden_proj): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=720, out_features=360, bias=True)\n",
      "  )\n",
      "  (encoder_cell_proj): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=720, out_features=360, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=720, out_features=18710, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "unk_idx, pad_idx, bos_idx, eos_idx = 0, 1, 2, 3\n",
    "\n",
    "train_loader = TrainDataLoader(train_dataset, shuffle=True)\n",
    "val_loader = TestDataLoader(val_dataset, shuffle=False)\n",
    "\n",
    "weights_filename = folders['weights'] + 'LSTM_3-testing-tf-metric-30.0m-15epoch.pt'\n",
    "\n",
    "model = LSTM_3(config=config, weights_filename=weights_filename).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.2534399 , 3.76737356, 3.56576598, 3.47964174, 3.42476594,\n",
       "       3.39220035, 3.40126699, 3.40977442, 3.38240802, 3.40251803,\n",
       "       3.41449302, 3.42746729, 3.42991304, 3.43507791, 3.43585044])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.load('../weights/train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission import get_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import RawDataset\n",
    "raw_dataset = RawDataset(filenames['test_src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:10<00:00,  2.66s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27.18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "get_bleu(model, dataloader=val_loader, vocab_trg=vocab_trg, filenames=filenames, device=device, raw_dataset=raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "val_loader = TestDataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "for src_seq, _ in val_loader:\n",
    "    batch_size = src_seq.size(1)\n",
    "    trg_seq = torch.tensor([[bos_idx]] * batch_size, dtype=torch.long).to(device)  # (batch_size, 1)\n",
    "    print(batch_size)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 21 20\n",
      "src:\t als ich <NUM> jahre alt war , wurde ich eines morgens von den klängen heller freude geweckt .\n",
      "trg:\t when i was <NUM> , i remember waking up one morning to the sound of joy in my house .\n",
      "pred:\t when i was <NUM> years old , i was by the sound of the sound of .\n",
      "17 15 17\n",
      "src:\t mein vater hörte sich auf seinem kleinen , grauen radio die der bbc an .\n",
      "trg:\t my father was listening to bbc news on his small , gray radio .\n",
      "pred:\t my father listened to his little , gray listening to the of the bbc .\n",
      "20 21 23\n",
      "src:\t er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens .\n",
      "trg:\t there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
      "pred:\t he looked very happy , which was pretty unusual at the time , because the news was most of the time .\n",
      "11 12 11\n",
      "src:\t er rief : \" die taliban sind weg ! \"\n",
      "trg:\t \" the taliban are gone ! \" my father .\n",
      "pred:\t he said , \" the taliban are gone ! \"\n",
      "20 22 21\n",
      "src:\t ich wusste nicht , was das bedeutete , aber es machte meinen vater offensichtlich sehr , sehr glücklich .\n",
      "trg:\t i didn 't know what it meant , but i could see that my father was very , very happy .\n",
      "pred:\t i didn 't know what that meant , but it was obviously very , very happy to my dad .\n",
      "21 27 23\n",
      "src:\t und ich fragte sie einmal und fragte sie erneut und bekam keine richtige antwort . nur leeres bla\n",
      "trg:\t and i asked them once , and i asked them again , and i got no real answer . it was only blah\n",
      "pred:\t and i asked her once and i got her back and got no real answer , just got a limb blah .\n",
      "24 22 25\n",
      "src:\t aber dann sagte ich mir , ich will diese informationen haben , weil es mein leben ist , das ihr da .\n",
      "trg:\t but then i said , i want to have this information , because this is my life you are .\n",
      "pred:\t but then i said , i want to have this information , because it 's my life you 're going to get there .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdemonstrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_src\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_trg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:14\u001b[39m, in \u001b[36mdemonstrate\u001b[39m\u001b[34m(self, val_loader, vocab_src, vocab_trg, examples, device, wait)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.demonstrate(val_loader, vocab_src, vocab_trg, device='cuda', examples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "sleep(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_submission = RawDataset(filenames['submission_src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m submission_dataset = SubmissionDataset(filenames[\u001b[33m'\u001b[39m\u001b[33msubmission_src\u001b[39m\u001b[33m'\u001b[39m], vocab_src, device=device)\n\u001b[32m      4\u001b[39m ldr = SubmissionDataLoader(submission_dataset)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmake_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mldr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_trg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_submission\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dubinin Daniil\\Desktop\\bhw2\\src\\submission.py:59\u001b[39m, in \u001b[36mmake_submission\u001b[39m\u001b[34m(model, submission_loader, vocab_trg, filenames, device, raw_dataset)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(submission_loader)):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         predictions = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# batch\u001b[39;00m\n\u001b[32m     60\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(a)):\n\u001b[32m     61\u001b[39m             t = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dubinin Daniil\\Desktop\\bhw2\\src\\lstm3.py:480\u001b[39m, in \u001b[36mLSTM_3.inference\u001b[39m\u001b[34m(self, src_seq, max_len, device)\u001b[39m\n\u001b[32m    476\u001b[39m mask = (src_seq != pad_idx).unsqueeze(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (batch_size, 1, src_len)\u001b[39;00m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    479\u001b[39m     \u001b[38;5;66;03m# encoder forward\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m     src_embedded = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msrc_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     encoder_outputs, (hidden, cell) = \u001b[38;5;28mself\u001b[39m.encoder(src_embedded) \u001b[38;5;66;03m# (B, max_len, H)\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# print('Encoder norm:', torch.norm(encoder_outputs[0, :20, :], 2))\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dubinin Daniil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dubinin Daniil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dubinin Daniil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dubinin Daniil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "from submission import make_submission\n",
    "from dataset import SubmissionDataset, SubmissionDataLoader\n",
    "submission_dataset = SubmissionDataset(filenames['submission_src'], vocab_src, device=device)\n",
    "ldr = SubmissionDataLoader(submission_dataset)\n",
    "make_submission(model, ldr, vocab_trg, filenames, device=device, raw_dataset=raw_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, trg = val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import TestDataLoader\n",
    "ldr = TestDataLoader(val_dataset, batch_size=2)\n",
    "src, trg = ldr[0]\n",
    "src.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  8125, 18184,     4, 18611,  8125, 18184, 18184, 11593, 10819,\n",
       "         10819, 10819, 10819, 10819, 16813, 11529, 16813, 11529,    32,    32,\n",
       "            32,    32,     3],\n",
       "        [    2,  6209, 18184, 11587,  7839,    28,  9758,    28,    28,  1543,\n",
       "            28,    28, 13407, 16813, 16813, 11529, 16813, 11529, 16813,    32,\n",
       "             3,     3,     3]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.inference(src, device='cpu')\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'was', '<NUM>', 'years', 'i', 'was']\n",
      "<BOS> i was <NUM> years i was was one morning morning morning morning morning the of the of . . . . <EOS> "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "trg_words = ['<BOS>', 'i', 'was', '<NUM>', 'years', 'i', 'was']\n",
    "i = len(trg_words) - 1\n",
    "trg_pred = torch.tensor([vocab_trg.encode_word(word) for word in trg_words], dtype=torch.long).unsqueeze(0)\n",
    "predictions = model.inference(src, device='cpu')\n",
    "print(vocab_trg.decode(trg_pred.squeeze(0).numpy()))\n",
    "for idx in predictions[0]:\n",
    "    print(vocab_trg.decode_idx(idx.item()), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'was',\n",
       " '<NUM>',\n",
       " 'years',\n",
       " 'i',\n",
       " 'was',\n",
       " 'was',\n",
       " 'one',\n",
       " 'morning',\n",
       " 'morning',\n",
       " 'morning',\n",
       " 'morning',\n",
       " 'morning',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'of',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_trg.decode(model.inference(src, device='cpu').squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    2,  8125, 18184,     4, 18611,  8125, 18184])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_pred.squeeze(0).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
