{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195915/195915 [00:30<00:00, 6455.14it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 6966.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'LSTM_3', 'feature': 'back-to-origins', 'max_len': 42, 'min_freq_src': 5, 'min_freq_trg': 5, 'embedding_dim': 192, 'hidden_size': 320, 'num_layers': 3, 'num_epochs': 15, 'weight_decay': 2e-05, 'label_smoothing': 0.1, 'dropout_emb': 0.2, 'dropout_enc': 0.4, 'dropout_dec': 0.4, 'dropout_attention': 0.15, 'learning_rate': 0.0008, 'gamma': 0.2, 'patience': 1, 'threshold': 0.0005, 'batch_size': 128, 'use_tf': False, 'tf_from_epoch': 0, 'tf_start': 0.9, 'tf_decrease': 0.02, 'src_vocab_size': 24991, 'trg_vocab_size': 18710}\n"
     ]
    }
   ],
   "source": [
    "print(config, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<dataset.Vocab at 0x2722527acf0>,\n",
       " <dataset.Vocab at 0x2727fa02490>,\n",
       " <dataset.TranslationDataset at 0x2722527ae40>,\n",
       " <dataset.TranslationDataset at 0x2722522b110>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_src, vocab_trg, train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset2 import TrainDataLoader, TestDataLoader\n",
    "from config import filenames, folders\n",
    "from lstm3 import LSTM_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_3(\n",
      "  (src_embedding): Embedding(24991, 192, padding_idx=1)\n",
      "  (trg_embedding): Embedding(18710, 192, padding_idx=1)\n",
      "  (emb_dropout): Dropout(p=0.2, inplace=True)\n",
      "  (enc_dropout): Dropout(p=0.4, inplace=True)\n",
      "  (dec_dropout): Dropout(p=0.4, inplace=True)\n",
      "  (attention_dropout): Dropout(p=0.15, inplace=True)\n",
      "  (encoder): LSTM(192, 320, num_layers=3, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "  (encoder_output_proj): Linear(in_features=640, out_features=320, bias=True)\n",
      "  (decoder): LSTM(192, 320, num_layers=3, batch_first=True, dropout=0.4)\n",
      "  (encoder_hidden_proj): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=640, out_features=320, bias=True)\n",
      "  )\n",
      "  (encoder_cell_proj): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=640, out_features=320, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=640, out_features=18710, bias=True)\n",
      ")\n",
      "30362262\n"
     ]
    }
   ],
   "source": [
    "unk_idx, pad_idx, bos_idx, eos_idx = 0, 1, 2, 3\n",
    "device = 'cuda'\n",
    "\n",
    "train_loader = TrainDataLoader(train_dataset, shuffle=True)\n",
    "val_loader = TestDataLoader(val_dataset, shuffle=False)\n",
    "\n",
    "weights_filename = folders['weights'] + 'LSTM_3-back-to-origins-30.0m-15epoch.pt'\n",
    "\n",
    "model = LSTM_3(config=config, weights_filename=weights_filename).to(device)\n",
    "print(model)\n",
    "print(model.count_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission import get_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:09<00:03,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:\t 9704 5192 3113 1896\n",
      "total:\t\t 14483 13497 12511 11533\n",
      "sinergy:\t32.04\n",
      "BP:\t\t0.673\n",
      "67.0/38.5/24.9/16.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(21.56)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import RawDataset\n",
    "raw_dataset = RawDataset(filenames['test_trg'])\n",
    "get_bleu(model, val_loader, vocab_trg, raw_dataset, True, 1, border=0.01/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_3(\n",
      "  (src_embedding): Embedding(24991, 192, padding_idx=1)\n",
      "  (trg_embedding): Embedding(18710, 192, padding_idx=1)\n",
      "  (emb_dropout): Dropout(p=0.2, inplace=True)\n",
      "  (enc_dropout): Dropout(p=0.4, inplace=True)\n",
      "  (dec_dropout): Dropout(p=0.4, inplace=True)\n",
      "  (attention_dropout): Dropout(p=0.15, inplace=True)\n",
      "  (encoder): LSTM(192, 320, num_layers=3, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "  (encoder_output_proj): Linear(in_features=640, out_features=320, bias=True)\n",
      "  (decoder): LSTM(192, 320, num_layers=3, batch_first=True, dropout=0.4)\n",
      "  (encoder_hidden_proj): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=640, out_features=320, bias=True)\n",
      "  )\n",
      "  (encoder_cell_proj): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=640, out_features=320, bias=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=640, out_features=18710, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:24<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "from submission import make_submission\n",
    "from dataset import SubmissionDataLoader, SubmissionDataset\n",
    "raw_dataset = RawDataset(filenames['submission_src'])\n",
    "submission_dataset = SubmissionDataset(filenames['submission_src'], vocab=vocab_src)\n",
    "submission_loader = SubmissionDataLoader(submission_dataset)\n",
    "make_submission(model, submission_loader=submission_loader, raw_dataset=raw_dataset, vocab_trg=vocab_trg, filenames=filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "src:             sie wollen die menschen in angst leben lassen .\n",
    "trg:             they want to make people live in fear .\n",
    "pred:            they want to live people in fear .\n",
    "pred-beam:       they want people to live in fear .\n",
    "sinergy:        37.15\n",
    "BP:             0.882\n",
    "100.0/57.1/33.3/10.0\n",
    "bleu: 32.78\n",
    "sinergy:        44.18\n",
    "BP:             0.882\n",
    "100.0/57.1/33.3/20.0\n",
    "bleu beam: 38.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:09<00:03,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:\t 11551 6301 3720 2242\n",
      "total:\t\t 17863 16877 15891 14913\n",
      "sinergy:\t30.36\n",
      "BP:\t\t0.876\n",
      "64.7/37.3/23.4/15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26.6"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = RawDataset(filenames['test_trg'])\n",
    "float(get_bleu(model, dataloader=val_loader, vocab_trg=vocab_trg, device=device, raw_dataset=raw_dataset, use_beam=True, beam_width=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### greedy\n",
    "sinergy:\t29.56\n",
    "\n",
    "BP:\t\t    0.955\n",
    "\n",
    "61.3/36.2/23.0/14.9\n",
    "\n",
    "correct:\t 11913 6661 4013 2459\n",
    "\n",
    "total:\t 19461 18475 17490 16510\n",
    "\n",
    "bleu: 28.34\n",
    "\n",
    "### beam\n",
    "sinergy:\t24.19\n",
    "\n",
    "BP:\t\t0.977\n",
    "\n",
    "54.7/30.3/18.3/11.3\n",
    "\n",
    "correct:\t 10826 5699 3255 1898\n",
    "\n",
    "total:\t 19774 18788 17803 16826\n",
    "\n",
    "\n",
    "bleu:   23.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission import bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, trg = val_dataset[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we were scared , but still , school was where we wanted to be .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab_trg.decode(model.inference(src.unsqueeze(0)).squeeze(0)))\n",
    "\" \".join(vocab_trg.decode(trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(11.726550850719802)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu(vocab_trg.decode(model.inference(src.unsqueeze(0)).squeeze(0)), vocab_trg.decode(trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src:\t\t als ich <NUM> jahre alt war , wurde ich eines morgens von den klängen heller freude geweckt .\n",
      "trg:\t\t when i was <NUM> , i remember waking up one morning to the sound of joy in my house .\n",
      "pred:\t\t when i was <NUM> years old , i was going to be able to get the message to the outside of the message .\n",
      "pred-beam:\t when i was <NUM> years old , one morning , i was one morning .\n",
      "sinergy:\t14.07\n",
      "BP:\t\t1\n",
      "41.7/21.7/9.1/4.8\n",
      "bleu:\t\t14.07\n",
      "sinergy:\t22.89\n",
      "BP:\t\t0.717\n",
      "60.0/35.7/15.4/8.3\n",
      "bleu beam:\t16.40\n",
      "\n",
      "src:\t\t mein vater hörte sich auf seinem kleinen , grauen radio die der bbc an .\n",
      "trg:\t\t my father was listening to bbc news on his small , gray radio .\n",
      "pred:\t\t my father was listening to his little , terrifying radio , the baby 's favorite of the bbc .\n",
      "pred-beam:\t my dad stopped\n",
      "sinergy:\t22.54\n",
      "BP:\t\t1\n",
      "52.6/22.2/17.6/12.5\n",
      "bleu:\t\t22.54\n",
      "sinergy:\t27.52\n",
      "BP:\t\t0.026\n",
      "33.3/25.0/25.0/0.0\n",
      "bleu beam:\t0.70\n",
      "\n",
      "src:\t\t er sah sehr glücklich aus , was damals ziemlich ungewöhnlich war , da ihn die nachrichten meistens .\n",
      "trg:\t\t there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
      "pred:\t\t he looked very happy , which was pretty unusual , because it was the news of the news .\n",
      "pred-beam:\t he looked very happy , which was pretty unusual at the time , which was pretty unusual at the time .\n",
      "sinergy:\t7.76\n",
      "BP:\t\t0.949\n",
      "47.4/16.7/2.9/1.6\n",
      "bleu:\t\t7.36\n",
      "sinergy:\t4.97\n",
      "BP:\t\t1\n",
      "33.3/5.0/2.6/1.4\n",
      "bleu beam:\t4.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.demonstrate(val_loader, vocab_src, vocab_trg, device='cuda', examples=3, wait=0, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(11.168140627821579)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from submission import bleu, get_precisions\n",
    "ref = \"there was a big smile on his face which was unusual then , because the news mostly depressed him .\"\n",
    "pred = \"he looked very happy , which was quite unusual , because it 's the news mostly .\"\n",
    "bleu(ref.split(), pred.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "pred_idx = model.inference(torch.tensor(vocab_src.encode(ref.split()), dtype=torch.long, device='cuda').unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'what', 'was', 'a', 'big', 'favorite']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_trg.decode(pred_idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9., 4., 1., 0.]), array([17., 16., 15., 14.]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_precisions(ref.split(), pred.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he looked very happy , which was quite unusual , because it 's the news mostly .\n",
      "there was a big smile on his face which was unusual then , because the news mostly depressed him .\n",
      "sinergy:\t13.32\n",
      "BP:\t\t0.838\n",
      "52.9/25.0/6.7/3.6\n",
      "custom:  11.168140627821579\n"
     ]
    }
   ],
   "source": [
    "from submission import bleu\n",
    "def export(filename, txt):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(txt)\n",
    "\n",
    "ref_filename = '../submission/a.txt'\n",
    "pred_filename = '../submission/b.txt'\n",
    "def get_bleu_txt(pred, ref):\n",
    "    export(pred_filename, pred)\n",
    "    export(ref_filename, ref)\n",
    "    print(pred)\n",
    "    print(ref)\n",
    "\n",
    "    print('custom: ', bleu(ref=ref.split(), c=pred.split(), verbose=True))\n",
    "    # print('correct: ', eval_bleu(pred_filename, ref_filename))\n",
    "\n",
    "get_bleu_txt(pred, ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m.demonstrate(val_loader, vocab_src, vocab_trg, examples=\u001b[32m5\u001b[39m, wait=\u001b[32m3\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.demonstrate(val_loader, vocab_src, vocab_trg, examples=5, wait=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_submission = RawDataset(filenames['submission_src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission import make_submission\n",
    "from dataset import SubmissionDataset, SubmissionDataLoader\n",
    "submission_dataset = SubmissionDataset(filenames['submission_src'], vocab_src, device=device)\n",
    "ldr = SubmissionDataLoader(submission_dataset)\n",
    "# make_submission(model, ldr, vocab_trg, filenames, device=device, raw_dataset=raw_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['als', 'ich', '<NUM>', 'jahre', 'alt', 'war', ',', 'wurde', 'ich', 'eines', 'morgens', 'von', 'den', 'klängen', 'heller', 'freude', 'geweckt', '.']\n"
     ]
    }
   ],
   "source": [
    "src, trg = val_dataset[0]\n",
    "print(vocab_src.decode(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = TestDataLoader(val_dataset, shuffle=False, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 5\tB: 3\n",
      "\n",
      "step:0\n",
      "beam tokens:\n",
      "index in batch: 0\n",
      "<BOS>\n",
      "<BOS>\n",
      "<BOS>\n",
      "<BOS>\n",
      "<BOS>\n",
      "\n",
      "index in batch: 1\n",
      "<BOS>\n",
      "<BOS>\n",
      "<BOS>\n",
      "<BOS>\n",
      "<BOS>\n",
      "\n",
      "index in batch: 2\n",
      "<BOS>\n",
      "<BOS>\n",
      "<BOS>\n",
      "<BOS>\n",
      "<BOS>\n",
      "\n",
      "beam probs:\n",
      "[[100.   0.   0.   0.   0.]\n",
      " [100.   0.   0.   0.   0.]\n",
      " [100.   0.   0.   0.   0.]]\n",
      "\n",
      "step:1\n",
      "beam tokens:\n",
      "index in batch: 0\n",
      "<BOS> when\n",
      "<BOS> and\n",
      "<BOS> by\n",
      "<BOS> at\n",
      "<BOS> i\n",
      "\n",
      "index in batch: 1\n",
      "<BOS> my\n",
      "<BOS> and\n",
      "<BOS> dad\n",
      "<BOS> the\n",
      "<BOS> i\n",
      "\n",
      "index in batch: 2\n",
      "<BOS> he\n",
      "<BOS> and\n",
      "<BOS> it\n",
      "<BOS> so\n",
      "<BOS> now\n",
      "\n",
      "beam probs:\n",
      "[[81.9  1.8  1.4  1.2  1.2]\n",
      " [81.9  5.   1.5  1.2  1.1]\n",
      " [54.9 16.5  4.5  2.2  1.4]]\n",
      "\n",
      "step:2\n",
      "beam tokens:\n",
      "index in batch: 0\n",
      "<BOS> when i\n",
      "<BOS> and when\n",
      "<BOS> by <NUM>\n",
      "<BOS> i was\n",
      "<BOS> at <NUM>\n",
      "\n",
      "index in batch: 1\n",
      "<BOS> my father\n",
      "<BOS> my dad\n",
      "<BOS> and my\n",
      "<BOS> my old\n",
      "<BOS> the father\n",
      "\n",
      "index in batch: 2\n",
      "<BOS> he looked\n",
      "<BOS> he saw\n",
      "<BOS> and he\n",
      "<BOS> he was\n",
      "<BOS> it looked\n",
      "\n",
      "beam probs:\n",
      "[[74.1  1.5  0.8  0.6  0.4]\n",
      " [49.7 20.2  4.1  0.6  0.6]\n",
      " [27.3 15.  12.2  4.5  2.2]]\n",
      "\n",
      "step:3\n",
      "beam tokens:\n",
      "index in batch: 0\n",
      "<BOS> when i was\n",
      "<BOS> and when i\n",
      "<BOS> i was <NUM>\n",
      "<BOS> by <NUM> ,\n",
      "<BOS> when i were\n",
      "\n",
      "index in batch: 1\n",
      "<BOS> my father was\n",
      "<BOS> my father complained\n",
      "<BOS> my father heard\n",
      "<BOS> my father had\n",
      "<BOS> and my father\n",
      "\n",
      "index in batch: 2\n",
      "<BOS> he looked very\n",
      "<BOS> he saw very\n",
      "<BOS> and he saw\n",
      "<BOS> and he looked\n",
      "<BOS> he looked really\n",
      "\n",
      "beam probs:\n",
      "[[74.1  1.4  0.6  0.5  0.3]\n",
      " [ 6.7  6.1  5.5  3.   2.7]\n",
      " [22.3 11.1  6.7  4.5  2.7]]\n",
      "\n",
      "step:4\n",
      "beam tokens:\n",
      "index in batch: 0\n",
      "<BOS> when i was <NUM>\n",
      "<BOS> when i was in\n",
      "<BOS> and when i was\n",
      "<BOS> when i was about\n",
      "<BOS> when i was a\n",
      "\n",
      "index in batch: 1\n",
      "<BOS> my father was clinging\n",
      "<BOS> and my father on\n",
      "<BOS> my father heard was\n",
      "<BOS> my father was preparing\n",
      "<BOS> my father complained on\n",
      "\n",
      "index in batch: 2\n",
      "<BOS> he looked very happy\n",
      "<BOS> he saw very happy\n",
      "<BOS> and he saw very\n",
      "<BOS> and he looked very\n",
      "<BOS> he looked really happy\n",
      "\n",
      "beam probs:\n",
      "[[60.7  1.4  1.2  0.8  0.6]\n",
      " [ 1.4  0.9  0.8  0.8  0.8]\n",
      " [18.3  9.1  5.   3.   2. ]]\n"
     ]
    }
   ],
   "source": [
    "for src, trg in val_loader:\n",
    "    model.inference_beam(src, verbose=True, max_len=5, vocab_trg=vocab_trg)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import TestDataLoader\n",
    "ldr = TestDataLoader(val_dataset, batch_size=2)\n",
    "src, trg = ldr[0]\n",
    "src.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'was', '<NUM>', 'years', 'i', 'was']\n",
      "<BOS> when i was <NUM> years old , i was told one morning by the bright hollywood joy , and i was behind <EOS> "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "trg_words = ['<BOS>', 'i', 'was', '<NUM>', 'years', 'i', 'was']\n",
    "i = len(trg_words) - 1\n",
    "trg_pred = torch.tensor([vocab_trg.encode_word(word) for word in trg_words], dtype=torch.long).unsqueeze(0)\n",
    "predictions = model.inference(src, device='cuda')\n",
    "print(vocab_trg.decode(trg_pred.squeeze(0).numpy()))\n",
    "for idx in predictions[0]:\n",
    "    print(vocab_trg.decode_idx(idx.item()), end=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
