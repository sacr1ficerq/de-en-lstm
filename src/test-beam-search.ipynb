{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm2 import train, LSTM_2\n",
    "\n",
    "from config import filenames, folders\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "config = {\n",
    "    'model_name': 'LSTM_2',\n",
    "    'feature': 'testing-tf-metric',\n",
    "    'max_len': 48,\n",
    "    'min_freq_src': 4,\n",
    "    'min_freq_trg': 4,\n",
    "\n",
    "    'embedding_dim': 128,\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 3,\n",
    "\n",
    "    'num_epochs': 10,\n",
    "    'weight_decay': 1e-5,\n",
    "    'label_smoothing': 0.1,\n",
    "\n",
    "    'dropout_enc': 0.1,\n",
    "    'dropout_dec': 0.1,\n",
    "    'dropout_emb': 0.1,\n",
    "    'dropout_attention': 0.1,\n",
    "\n",
    "    'learning_rate': 1e-3,\n",
    "    'gamma': 0.2,\n",
    "    'patience': 2,\n",
    "    'threshold': 5e-4,\n",
    "    'batch_size': 128,\n",
    "\n",
    "    'use_tf': False,\n",
    "    'tf_start': 1e+10,\n",
    "    'tf_decrease': 1e+10\n",
    "}\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Vocab\n",
    "vocab_src = Vocab(filenames['train_src'], min_freq=config['min_freq_src'])\n",
    "vocab_trg = Vocab(filenames['train_trg'], min_freq=config['min_freq_trg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29798\n",
      "21555\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_src))\n",
    "print(len(vocab_trg))\n",
    "config['src_vocab_size'] = len(vocab_src)\n",
    "config['trg_vocab_size'] = len(vocab_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195915/195915 [00:13<00:00, 14677.17it/s]\n",
      "100%|██████████| 986/986 [00:00<00:00, 16056.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from dataset import TranslationDataset\n",
    "train_dataset = TranslationDataset(vocab_src, \n",
    "                                vocab_trg, \n",
    "                                filenames['train_src'], \n",
    "                                filenames['train_trg'], \n",
    "                                max_len=config['max_len'], \n",
    "                                device=device)\n",
    "val_dataset = TranslationDataset(vocab_src, \n",
    "                                vocab_trg, \n",
    "                                filenames['test_src'], \n",
    "                                filenames['test_trg'], \n",
    "                                max_len=72, \n",
    "                                device=device, \n",
    "                                sort_lengths=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_2(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load('../weights/lstm2/LSTM_2-more-dropouts-23.0m-15epoch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission import get_bleu\n",
    "from dataset import TestDataLoader, RawDataset\n",
    "\n",
    "val_loader = TestDataLoader(val_dataset, batch_size=2)\n",
    "\n",
    "raw = RawDataset(filenames['test_src'])\n",
    "\n",
    "# get_bleu(model, val_loader, vocab_trg, filenames, device=device, raw_dataset=raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 35])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_seq = None\n",
    "trg_seq = None\n",
    "for a, b in val_loader:\n",
    "    src_seq = a\n",
    "    trg_seq = b\n",
    "src_seq.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "unk_idx, pad_idx, bos_idx, eos_idx, num_idx = 0, 1, 2, 3, 4\n",
    "\n",
    "self = model\n",
    "beam_width = 3\n",
    "bos_idx = 2\n",
    "\n",
    "model.eval()\n",
    "batch_size = src_seq.size(0)\n",
    "k = beam_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder forward\n",
    "with torch.no_grad():\n",
    "    src_embedded = self.src_embedding(src_seq)  # (batch_size, seq_len, emb_dim)\n",
    "    encoder_outputs, (hidden, cell) = self.encoder(src_embedded)\n",
    "    encoder_outputs = self.encoder_output_proj(encoder_outputs)  # (batch_size, seq_len, hidden_dim)\n",
    "    encoder_outputs = encoder_outputs.contiguous()\n",
    "\n",
    "    hidden = self._project_hidden(hidden, self.encoder_hidden_proj)  # (num_layers, batch_size, hidden_dim)\n",
    "    cell = self._project_hidden(cell, self.encoder_cell_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 35, 256])\n"
     ]
    }
   ],
   "source": [
    "# expand encoder output for beams\n",
    "encoder_outputs = encoder_outputs.unsqueeze(1).repeat(1, k, 1, 1)  # (batch_size, k, seq_len, hidden_dim)\n",
    "print(encoder_outputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 35, 256])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_outputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 6, 256]) torch.Size([3, 6, 256])\n"
     ]
    }
   ],
   "source": [
    "hidden = hidden.unsqueeze(2).repeat(1, 1, k, 1)  # (num_layers, batch_size, k, hidden_dim)\n",
    "hidden = hidden.view(hidden.size(0), -1, hidden.size(-1))  # (num_layers, batch_size*k, hidden_dim)\n",
    "cell = cell.unsqueeze(2).repeat(1, 1, k, 1).view(hidden.size(0), -1, hidden.size(-1))\n",
    "print('Hidden sizes: ', hidden.size(), cell.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00, -1.0000e+09, -1.0000e+09],\n",
       "        [ 0.0000e+00, -1.0000e+09, -1.0000e+09]])"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_scores = torch.zeros((batch_size, k), dtype=torch.float, device=device)  # (batch_size, k)\n",
    "beam_scores[:, 1:] = -1e10  # force beam 0 to be top 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam scores: tensor([ 0.0000e+00, -1.0000e+09, -1.0000e+09]) tensor([ 0.0000e+00, -1.0000e+09, -1.0000e+09])\n",
      "Beam tokens: tensor([[2],\n",
      "        [2],\n",
      "        [2]]) tensor([[2],\n",
      "        [2],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize Beams ---\n",
    "beam_scores = torch.zeros((batch_size, k), dtype=torch.float, device=device)  # (batch_size, k)\n",
    "beam_scores[:, 1:] = -1e10  # force beam 0 to be top 1\n",
    "print('Beam scores:', *beam_scores)\n",
    "\n",
    "beam_tokens = torch.full((batch_size, k, 1), bos_idx, dtype=torch.long, device=device)  # (batch_size, k, 1)\n",
    "print('Beam tokens:', *beam_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_hidden = hidden  # (num_layers, batch_size*k, hidden_dim)\n",
    "flat_cell = cell\n",
    "flat_tokens = beam_tokens.view(batch_size * k, -1)  # (batch_size*k, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat hidd size:\t torch.Size([3, 6, 256])\n",
      "Flat cell size:\t torch.Size([3, 6, 256])\n",
      "Flat tokens size:\t torch.Size([6, 3])\n",
      "Flat tokens:\t tensor([[    2,  9410, 20904],\n",
      "        [    2,  9410,    15],\n",
      "        [    2,   857,  9410],\n",
      "        [    2, 19336, 21475],\n",
      "        [    2, 19341, 20657],\n",
      "        [    2, 17795, 19336]])\n"
     ]
    }
   ],
   "source": [
    "print('Flat hidd size:\\t', flat_hidden.size())\n",
    "print('Flat cell size:\\t', flat_cell.size())\n",
    "print('Flat tokens size:\\t', flat_tokens.size())\n",
    "print('Flat tokens:\\t', flat_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "current_trg = flat_tokens[:, -1].unsqueeze(1)  # (batch_size*k, 1)\n",
    "\n",
    "trg_embedded = self.trg_embedding(current_trg)  # (batch_size*k, 1, emb_dim)\n",
    "\n",
    "decoder_output, (new_hidden, new_cell) = self.decoder(trg_embedded, (flat_hidden, flat_cell))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current trg: tensor([[20904],\n",
      "        [   15],\n",
      "        [ 9410],\n",
      "        [21475],\n",
      "        [20657],\n",
      "        [19336]])\n"
     ]
    }
   ],
   "source": [
    "print('Current trg:', current_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 35])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 35])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (src_seq != pad_idx).unsqueeze(1)  # (batch_size, 1, src_len)\n",
    "print(mask.size())\n",
    "mask = mask.repeat(k, 1, 1)\n",
    "mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention + logits\n",
    "energy = torch.bmm(decoder_output, encoder_outputs.transpose(1, 2))  # (batch_size*k, 1, seq_len)\n",
    "\n",
    "energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "\n",
    "attention = F.softmax(energy, dim=-1)\n",
    "context = torch.bmm(attention, encoder_outputs)  # (batch_size*k, 1, hidden_dim)\n",
    "combined = torch.cat([decoder_output, context], dim=2)\n",
    "logits = self.fc(combined).squeeze(1)  # (batch_size*k, vocab_size)\n",
    "logits[:, unk_idx] = -1e10  # Block <UNK>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits size:\t torch.Size([6, 21555])\n"
     ]
    }
   ],
   "source": [
    "print('logits size:\\t', logits.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ich möchte jeden von ihnen heute dazu anregen , mit in das bild zu kommen und zögern sie nicht jemanden zu fragen : \" würden sie ein bild von uns machen ? \"\n",
      "vielen dank .\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(vocab_src.decode(src_seq[0])))\n",
    "print(\" \".join(vocab_src.decode(src_seq[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# scores (log probs)\n",
    "log_probs = F.log_softmax(logits, dim=-1)  # (batch_size*k, vocab_size)\n",
    "vocab_size = log_probs.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 21555])"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float32(85.88), 'to') (np.float32(1.3), 'everybody') (np.float32(1.23), 'you')\n"
     ]
    }
   ],
   "source": [
    "seq_n = 0\n",
    "topk = torch.topk(log_probs, k, dim=-1)\n",
    "probs = np.round(np.exp(topk.values.detach().numpy()[seq_n])*100, 2)\n",
    "candidates = vocab_trg.decode(topk.indices[seq_n])\n",
    "print(*zip(probs, candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.07818   ]\n",
      " [-2.709785  ]\n",
      " [-3.2239377 ]\n",
      " [-0.32311833]\n",
      " [-3.8356533 ]\n",
      " [-4.241947  ]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(beam_scores.view(-1, 1).detach().numpy())\n",
    "print(np.round(np.exp(log_probs.detach().numpy()), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1])\n",
      "torch.Size([6, 21555])\n"
     ]
    }
   ],
   "source": [
    "print(beam_scores.view(-1, 1).size())\n",
    "print(log_probs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_scores = log_probs + beam_scores.view(-1, 1)  # (batch_size*k, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 21555])\n"
     ]
    }
   ],
   "source": [
    "print(next_scores.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64665])"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape to (batch_size, k * vocab_size)\n",
    "next_scores = next_scores.view(batch_size, k * vocab_size)\n",
    "next_scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topk candidates for each batch \n",
    "next_scores, next_tokens = torch.topk(next_scores, k, dim=1)  # log_probs: (batch_size, k), indices: (batch_size, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs: [[0.29 0.03 0.01]\n",
      " [0.33 0.28 0.02]]\n",
      "next tokens: tensor([[19572, 64014, 27594],\n",
      "        [   35, 20657, 17795]])\n"
     ]
    }
   ],
   "source": [
    "print('probs:', np.round(np.exp(next_scores.detach().numpy()), 2))\n",
    "print('next tokens:', next_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_indices = next_tokens // vocab_size  # what beam is token from\n",
    "token_indices = next_tokens % vocab_size  # what token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2, 1],\n",
      "        [0, 0, 0]])\n",
      "tensor([[19572, 20904,  6039],\n",
      "        [   35, 20657, 17795]])\n"
     ]
    }
   ],
   "source": [
    "print(beam_indices)\n",
    "print(token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch source sequernces:\n",
      "ich möchte jeden von ihnen heute dazu anregen , mit in das bild zu kommen und zögern sie nicht jemanden zu fragen : \" würden sie ein bild von uns machen ? \"\n",
      "vielen dank .\n",
      "\n",
      "Candidates for position 2:\n",
      "['to', 'want', 'each']\n",
      "['.', 'very', 'so']\n",
      "Probabilities for position 2:\n",
      "[0.29229257 0.02930492 0.00927902]\n",
      "[0.33621648 0.28083163 0.024972  ]\n"
     ]
    }
   ],
   "source": [
    "print('Batch source sequernces:')\n",
    "print(\" \".join(vocab_src.decode(src_seq[0])))\n",
    "print(\" \".join(vocab_src.decode(src_seq[1])))\n",
    "print()\n",
    "print(f'Candidates for position {step}:')\n",
    "print(*map(vocab_trg.decode, token_indices.detach().numpy()), sep='\\n')\n",
    "print(f'Probabilities for position {step}:')\n",
    "print(*map(np.exp, np.round(next_scores.detach().numpy(), 2)), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### new beam states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new scores\n",
    "beam_scores = next_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2304108 -3.5302777 -4.676137 ]\n",
      " [-1.0944414 -1.2699423 -3.6939917]]\n"
     ]
    }
   ],
   "source": [
    "print(beam_scores.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new beam tokens\n",
    "beam_tokens = torch.cat([\n",
    "    beam_tokens[torch.arange(batch_size).unsqueeze(1), beam_indices],  # (batch_size, k) -> (batch_size, k, seq_len)\n",
    "    token_indices.unsqueeze(-1)\n",
    "], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[    2,  9410, 20904, 19572],\n",
      "         [    2,   857,  9410, 20904],\n",
      "         [    2,  9410,    15,  6039]],\n",
      "\n",
      "        [[    2, 19336, 21475,    35],\n",
      "         [    2, 19336, 21475, 20657],\n",
      "         [    2, 19336, 21475, 17795]]])\n"
     ]
    }
   ],
   "source": [
    "print(beam_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beams for seq0 in batch:\n",
      "['i', 'want', 'to']\n",
      "['and', 'i', 'want']\n",
      "['i', \"'d\", 'each']\n",
      "beam probabilities:\n",
      "0.29217252\n",
      "0.02929678\n",
      "0.009314928\n",
      "\n",
      "beams for seq1 in batch:\n",
      "['thank', 'you', '.']\n",
      "['thank', 'you', 'very']\n",
      "['thank', 'you', 'so']\n",
      "beam probabilities:\n",
      "0.33472654\n",
      "0.28084785\n",
      "0.024872521\n"
     ]
    }
   ],
   "source": [
    "print('beams for seq0 in batch:')\n",
    "print(*map(vocab_trg.decode, beam_tokens[0, :].numpy()), sep='\\n')\n",
    "print('beam probabilities:')\n",
    "print(*map(np.exp, beam_scores[0, :].detach().numpy()), sep='\\n')\n",
    "\n",
    "print('\\nbeams for seq1 in batch:')\n",
    "print(*map(vocab_trg.decode, beam_tokens[1, :].numpy()), sep='\\n')\n",
    "print('beam probabilities:')\n",
    "print(*map(np.exp, beam_scores[1, :].detach().numpy()), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = hidden.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 3, 256])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_hidden.view(num_layers, batch_size, k, -1).size()  # (num_layers, batch_size, k, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new hidden states\n",
    "hidden = new_hidden.view(num_layers, batch_size, k, -1)  # (num_layers, batch_size, k, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 3, 256])"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 3, 256])\n"
     ]
    }
   ],
   "source": [
    "print(hidden.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = hidden.view(hidden.size(0), -1, hidden.size(-1))  # (num_layers, batch_size*k, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = new_cell.view(num_layers, batch_size, k, -1)  # (num_layers, batch_size, k, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = cell.view(cell.size(0), -1, cell.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Update hidden/cell states\n",
    "\n",
    "# cell = new_cell.view(cell.size(0), batch_size, k, -1)\n",
    "# cell = cell[torch.arange(batch_size).unsqueeze(1), beam_indices].view(cell.size(0), -1, cell.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Check for EOS ---\n",
    "eos_mask = (token_indices == eos_idx)\n",
    "if eos_mask.all():\n",
    "    # Store finished beams and adjust active beams\\\n",
    "    pass\n",
    "    # break  # (Implement early stopping logic here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  9410, 20904, 19572, 19048,  6743, 13293,   684,  9410, 20904,\n",
       "         19572, 19048,   978, 13293, 21089,  9410, 20904, 19572, 19048,  6743,\n",
       "         13293,   684,  9410, 20904, 19572, 19048,   978, 13293, 21089,  9410,\n",
       "         20904, 19572, 19048,  6743, 13293,   684,  9410, 20904, 19572, 19048,\n",
       "           978, 13293, 21089,  9410, 20904, 19572, 19048,   978, 13293, 20904,\n",
       "         19572, 19048,   978, 13293],\n",
       "        [    2, 19336, 21475,    35,     3,    35,     3,    35,     3,    35,\n",
       "             3,    35,     3,    35,     3,    35,     3,    35,     3,    35,\n",
       "             3,    35,     3,    35,     3,    35,     3,    35,     3,    35,\n",
       "             3,    35,     3,    35,     3,    35,     3,    35,     3,    35,\n",
       "             3,    35,     3,    35,     3,    35,     3,    35,     3,    35,\n",
       "             3,    35,     3,    35]])"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_seq = beam_tokens[torch.arange(batch_size), beam_scores.argmax(dim=1)]  # (batch_size, seq_len)\n",
    "trg_seq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
